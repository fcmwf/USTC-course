{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以电影简介为例比较jieba和snownlp的处理准确度和处理时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import pickle\n",
    "import csv\n",
    "import re\n",
    "from snownlp import SnowNLP\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intro_process(text):\n",
    "    global stop_words\n",
    "    global alltag\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # 分割句子\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    # 去除多余的空白\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if len(sentences)!=0:\n",
    "        sentences=sentences[0].split()\n",
    "    originaltag=[]\n",
    "    origin=[]\n",
    "    for sentence in sentences:\n",
    "        originaltag=jieba.lcut(sentence)\n",
    "        origin.append(originaltag)\n",
    "    if len(origin)>0:\n",
    "        origin=origin[0]\n",
    "    kong=[' ']\n",
    "    afterdelte=[]\n",
    "    for word in origin:\n",
    "        if word not in stop_words and word not in kong and word in alltag:\n",
    "            afterdelte.append(word)\n",
    "    return afterdelte\n",
    "def snow_process(text):\n",
    "    global stop_words\n",
    "    global alltag\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # 分割句子\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    # 去除多余的空白\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if len(sentences)!=0:\n",
    "        sentences=sentences[0].split()\n",
    "    originaltag=[]\n",
    "    origin=[]\n",
    "    for sentence in sentences:\n",
    "        originaltag=SnowNLP(sentence).words\n",
    "        origin.append(originaltag)\n",
    "    if len(origin)>0:\n",
    "        origin=origin[0]\n",
    "    kong=[' ']\n",
    "    afterdelte=[]\n",
    "    for word in origin:\n",
    "        if word not in stop_words and word not in kong and word in alltag:\n",
    "            afterdelte.append(word)\n",
    "    return afterdelte \n",
    "  \n",
    "stop_list='../data/stopword.txt'\n",
    "stop_words = []\n",
    "with open(stop_list, 'r', encoding='utf-8') as stoplst:\n",
    "    for line in stoplst.readlines():\n",
    "        stop_words.append(line.strip())\n",
    "neededinfo=[]\n",
    "alltag=[]\n",
    "filepath='../data/movie.csv'\n",
    "with open(filepath, 'r',encoding='utf-8') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        neededinfo.append(row)\n",
    "for tag in neededinfo:\n",
    "    tags=tag_process(tag[5])\n",
    "    for ta in tags:\n",
    "        if ta not in alltag:\n",
    "            alltag.append(ta)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## jieba从简介所得到的tag总数和花费时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n",
      "1.6928455829620361\n"
     ]
    }
   ],
   "source": [
    "jiebatag=[]\n",
    "start=time.time()\n",
    "for data in neededinfo:\n",
    "    jiebatag+=intro_process(data[13])\n",
    "end=time.time()\n",
    "print(len(jiebatag))\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## snownlp从电影简介所得到的tag总数和花费时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "64.91031289100647\n"
     ]
    }
   ],
   "source": [
    "snowtag=[]\n",
    "start=time.time()\n",
    "for data in neededinfo:\n",
    "    snowtag+=snow_process(data[13])\n",
    "end=time.time()\n",
    "print(len(snowtag))\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "jieba花费时间1.69s,得到tag数206\n",
    "snownlp花费时间64.91s,得到tag数238\n",
    "可见jieba花费时间少,精度较低,snownlp花费时间远大于jieba,精度更高"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
