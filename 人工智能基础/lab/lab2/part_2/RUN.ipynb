{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4460/4460 [06:58<00:00, 10.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 3.2044285891301962\n",
      "Epoch 0 Loss: 2.824110276816672\n",
      "Epoch 0 Train Loss: 3.2044285891301962, Valid Loss: 2.824110276816672\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 463\u001b[0m\n\u001b[0;32m    458\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    460\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m--> 463\u001b[0m     \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI could pick my lance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m )\n",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m, in \u001b[0;36mTokenizer.decode\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     tokens : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     47\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    TODO:\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    input : Tensor([0,1,2,3]) \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    output : \"ABCD\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex2char\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     46\u001b[0m     tokens : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m     47\u001b[0m     ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    TODO:\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    input : Tensor([0,1,2,3]) \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m    output : \"ABCD\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex2char\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens])\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sentence\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataPath:str\n",
    "        ):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(\n",
    "        self,\n",
    "        ):\n",
    "\n",
    "        unique_chars = sorted(set(self.dataset))\n",
    "        self.char2index = {char: idx + 1 for idx, char in enumerate(unique_chars)}  # start tokens from 1\n",
    "        self.char2index['<pad>'] = 0  # padding token\n",
    "        self.index2char = {idx: char for char, idx in self.char2index.items()}\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        sentence : str,\n",
    "        ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3]) \n",
    "\n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        tokens = [self.char2index.get(char, 0) for char in sentence]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tokens : torch.Tensor,\n",
    "        ) -> str:\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3]) \n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        sentence = ''.join([self.index2char[token] for token in tokens])\n",
    "        return sentence\n",
    "    \n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        text = text[: int(len(text) / 20)]\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #TODO: 提取一段文本(长度为 chunk_size）作为输入，以及这段文本的每一个字符的下一个字符作为标签\n",
    "        # example(not correspond to real text): chunk = tensor([ 0, 20, 49, 58, 59])\n",
    "        #                                       label = tensor([20, 49, 58, 59, 19])\n",
    "        # decoded chunk: \"The \"\n",
    "        # decoded label: \"he T\"\n",
    "        chunk = self.encoded[idx:idx + self.chunk_size]\n",
    "        label = self.encoded[idx + 1:idx + self.chunk_size + 1]\n",
    "        \n",
    "        return chunk, label\n",
    "\n",
    "class HeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_len:int, embed_size:int, hidden_size:int):\n",
    "        super().__init__()\n",
    "        # embed_size: dimension for input embedding vector\n",
    "        # hidden_size: dimension for hidden vector. eg. x:(..., embed_size) --to_q--> query_vector:(..., hidden_size)\n",
    "\n",
    "        # TODO: init three matrix, to_q, to_k, to_v.\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # 初始化权重矩阵\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "        # a triangular bool matrix for mask\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size)\n",
    "        # return (batch_size, seq_len, hidden_size)\n",
    "        # TODO: implement the attention mechanism\n",
    "        inputs.to(device)\n",
    "        # 计算 query, key, value\n",
    "        query = self.to_q(inputs)  # (batch_size, seq_len, hidden_size)\n",
    "        key = self.to_k(inputs)    # (batch_size, seq_len, hidden_size)\n",
    "        value = self.to_v(inputs)  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        # Q 和 K 的乘积\n",
    "        scores = torch.bmm(query, key.transpose(1, 2))  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # 对 scores 进行缩放处理\n",
    "        scores = scores / (self.hidden_size ** 0.5)\n",
    "        \n",
    "        # 对 scores 进行屏蔽，将上三角部分置为负无穷，避免模型在注意力计算时泄露未来信息\n",
    "        scores = scores.masked_fill(self.tril == 0, float('-inf'))\n",
    "        \n",
    "        # 对 scores 进行 softmax 操作，得到注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # 将注意力权重与 value 相乘得到注意力输出\n",
    "        attention_output = torch.bmm(attention_weights, value).to(inputs.device)  # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return attention_output \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # MultiHeadAttention is consist of many HeadAttention output.\n",
    "    # concat all this head attention output o_i, then merge them with a projection matrix W_o, as [o_1, o_2, ...] x W_o\n",
    "    # The reason for using multi-head attention is that we want each head to be able to extract different features\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        # n_heads is the number of head attention\n",
    "        # head_size is the hidden_size in each HeadAttention\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.head_size = head_size\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        \n",
    "        # Calculate head_size\n",
    "        head_size = embed_size // n_heads\n",
    "        \n",
    "        # Initialize multiple HeadAttention instances\n",
    "        self.heads = nn.ModuleList([\n",
    "            HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)\n",
    "            ])\n",
    "        \n",
    "        # Projection matrix\n",
    "        self.projection = nn.Linear(n_heads * head_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.projection.weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # input: (batch_size, seq_len, embed_size), make sure embed_size=n_heads x head_size\n",
    "        # return: (batch_size, seq_len, embed_size)\n",
    "        # TODO:\n",
    "        inputs.to(device)\n",
    "        head_outputs = [\n",
    "            head(inputs) for head in self.heads\n",
    "            ]  # List of tensors of shape (batch_size, seq_len, head_size)\n",
    "        \n",
    "        # Concatenate along the feature dimension\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)  # (batch_size, seq_len, n_heads * head_size)\n",
    "        \n",
    "        # Project back to embed_size\n",
    "        projected = self.projection(concatenated).to(inputs.device)  # (batch_size, seq_len, embed_size)\n",
    "        \n",
    "        return projected\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size:int):\n",
    "        super().__init__()\n",
    "        #TODO: init two linear layer\n",
    "        self.linear1 = nn.Linear(embed_size, 4 * embed_size)\n",
    "        self.linear2 = nn.Linear(4 * embed_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.linear1.weight)\n",
    "        nn.init.xavier_uniform_(self.linear2.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        # -> mid: (batch_size, seq_len, 4 * embed_size)\n",
    "        # -> outputs: (batch_size, seq_len, embed_size)\n",
    "        inputs.to(device)\n",
    "        mid = self.linear1(inputs)  # (batch_size, seq_len, 4 * embed_size)\n",
    "        mid = F.relu(mid)  # 使用ReLU作为非线性变换\n",
    "        \n",
    "        outputs = self.linear2(mid).to(inputs.device)  # (batch_size, seq_len, embed_size)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        super().__init__() \n",
    "        ## TODO\n",
    "        ## embed_size : dimension of embedding \n",
    "        ## num_experts : how many Experts per layer\n",
    "        ## active_experts: only active_experts out of num_experts are selected to process Embeddings per token.\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        \n",
    "        # MLP for computing scores\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, num_experts)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        ## 完成这部分时，注意使用Softmax()对router_output做标准化。同时注意这部分所用操作的可导性。\n",
    "        ## 输入值\n",
    "        ## inputs is the output tensor from multihead self attention block, shape (B:batch size, T: seq_len, C: embed_size)\n",
    "        ## 返回值\n",
    "        ## router_output: normalized weight of Experts, 即教程中的 \\alpha\n",
    "        ## indices:   index of selected Experts, 即教程中的 index\n",
    "        inputs.to(device)\n",
    "        score = self.mlp(inputs)\n",
    "        _ , indices = torch.topk(score, self.active_experts, dim=-1)\n",
    "        mask = torch.zeros_like(score, dtype=torch.bool)\n",
    "        mask.scatter_(dim=-1, index=indices, value=True)\n",
    "\n",
    "        score = score.masked_fill(~mask, float(\"-inf\"))\n",
    "        router_output = F.softmax(score, dim=-1).to(inputs.device)\n",
    "        return router_output, indices\n",
    "    \n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        \n",
    "        # Initialize TopkRouter and Experts\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        \n",
    "        # Softmax layer for combining outputs from active experts\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs.to(device)\n",
    "        # inputs: (batch_size, seq_len, embed_size)\n",
    "        \n",
    "        # Get routing information from TopkRouter\n",
    "        router_output, indices = self.router(inputs)  # router_output: (batch_size, seq_len, num_experts), indices: (batch_size, seq_len, active_experts)\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        batch_size, seq_len, embed_size = inputs.size()\n",
    "        final_output = torch.zeros_like(inputs).to(inputs.device)\n",
    "        \n",
    "        # Iterate through each token and apply selected experts\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                selected_experts = [self.experts[idx] for idx in indices[i, j]]  # Select experts based on indices\n",
    "                selected_weights = router_output[i, j, indices[i, j]]  # Weights corresponding to selected experts\n",
    "                \n",
    "                # Compute weighted sum of outputs from selected experts\n",
    "                expert_outputs = torch.cat([expert(inputs[i:i+1, j:j+1, :]) for expert in selected_experts], dim=0)  # List to tensor\n",
    "                weighted_sum = torch.sum(expert_outputs * selected_weights.view(-1, 1, 1), dim=0)\n",
    "                \n",
    "                # Assign to final output tensor\n",
    "                final_output[i, j, :] = weighted_sum\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # Transformer basic block, consist of MultiHeadAttention, FeedForward and layer normalization\n",
    "    def __init__(self, embed_size:int, n_heads:int, seq_len:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(\n",
    "            n_heads, embed_size // n_heads, seq_len, embed_size\n",
    "        )\n",
    "        self.moe = SparseMoE(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x = x + self.attention(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.moe(x)\n",
    "        return x\n",
    "\n",
    "class SparseMoETransformer(nn.Module):\n",
    "    def __init__(self, vocab_size:int, seq_len:int, embed_size:int, n_layers:int, n_heads:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # Token embedding and position embedding\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        \n",
    "        # Stack of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_size, n_heads, seq_len, num_experts, active_experts) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization and output linear layer\n",
    "        self.layernorm = nn.LayerNorm(embed_size)\n",
    "        self.output_linear = nn.Linear(embed_size, vocab_size)\n",
    "        nn.init.xavier_uniform_(self.output_linear.weight)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        batch_size, seq_len = inputs.shape\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.token_embedding(inputs)  # (batch_size, seq_len, embed_size)\n",
    "        position_indices = torch.arange(seq_len, device=inputs.device)\n",
    "        position_embeddings = self.position_embedding(position_indices)  # (1, seq_len, embed_size)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "        \n",
    "        # Pass through the stack of Transformer blocks\n",
    "        x = embeddings\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "        \n",
    "            # Layer normalization and linear layer\n",
    "        x = self.layernorm(x)\n",
    "        logits = self.output_linear(x)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = torch.tensor(tokenizer.encode(inputs)).unsqueeze(0)\n",
    "        device = next(self.parameters()).device  \n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, :self.seq_len]\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            \n",
    "            \n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]  \n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)  \n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)  \n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)\n",
    "        return generated\n",
    "\n",
    "    \n",
    "def train(model, dataloader, epoch, device):\n",
    "    # Optimizer 会根据模型的输出和真实标签计算梯度，然后利用反向传播算法更新模型的参数。\n",
    "    # 在本实验中你可以将 Optimizer 视作黑盒，只需要知道如何使用即可。\n",
    "    # 找一个合适的 Optimizer。对不同的任务，模型，最适合的优化器是不一样的，你可以先尝试最常用的 Adam，如果有兴趣可以看看其他的优化器。\n",
    "    # docs see: https://pytorch.org/docs/stable/optim.html \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # 选择 Adam 作为优化器\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.clone().detach()\n",
    "        targets = targets.clone().detach()\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch} Loss: {avg_loss}')\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss / len(dataloader)}\")\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset,val_dataset = torch.utils.data.random_split(dataset,[int(len(dataset)*0.8),len(dataset)-int(len(dataset)*0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=0):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        print(f\"Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "\n",
    "def plot_loss(train_loss, valid_loss):\n",
    "    plt.plot(train_loss, label=\"train loss\")\n",
    "    plt.plot(valid_loss, label=\"valid loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "train_dataloader, val_dataloader = create_dataloader(\n",
    "    \"input.txt\",\n",
    "    tokenizer,\n",
    "    chunk_size=20,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "model = SparseMoETransformer(\n",
    "                vocab_size=len(tokenizer.char2index), \n",
    "                seq_len=20, \n",
    "                embed_size=16, \n",
    "                n_layers=1, \n",
    "                n_heads=1, \n",
    "                num_experts=1, \n",
    "                active_experts=1\n",
    "        ).to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_losses, valid_losses = run(\n",
    "    model, train_dataloader, val_dataloader, device, epochs=1\n",
    ")\n",
    "\n",
    "plot_loss(train_losses, valid_losses)\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(\"I could pick my lance\",max_new_tokens=100)[0].tolist()\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
