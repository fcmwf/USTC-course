{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:46.927955Z",
     "start_time": "2024-06-22T02:51:46.919365Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLao8DKovBQe"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为 Token 序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "\n",
    "-   字符级：将文本分解为字符。\n",
    "-   单词级：将文本分解为单词。\n",
    "-   子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和 Tokens 序列之间相互转化的函数，即可完成 Tokenization 部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单 tokenizer， 由 字符表， 字符到 token 的 encoder()函数 和 token 到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的 tokenizer 实现，比如 openai 的 tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:46.988217Z",
     "start_time": "2024-06-22T02:51:46.979940Z"
    },
    "id": "EgEOwXJ1vBQf"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, dataPath: str):\n",
    "        with open(dataPath, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "        self.generate_vocabulary()\n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        index = 0\n",
    "        for char in self.dataset:\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index] = char\n",
    "                index += 1\n",
    "\n",
    "    def encode(self, sentence: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input  : \"ABCD\"\n",
    "        output : Tensor([0,1,2,3])\n",
    "\n",
    "        注意: 为了后续实验方便，输出Tensor的数据类型dtype 为torch.long。\n",
    "        \"\"\"\n",
    "        return torch.tensor(\n",
    "            [self.char2index[char] for char in sentence], dtype=torch.long\n",
    "        )\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        \"\"\"\n",
    "        例子, 假设A-Z 对应的token是1-26, 句子开始，结束符号的token是0。\n",
    "        input : Tensor([0,1,2,3])\n",
    "        output : \"ABCD\"\n",
    "        \"\"\"\n",
    "        return \"\".join([self.index2char[index] for index in tokens])\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvbipIfAvBQf"
   },
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.075160Z",
     "start_time": "2024-06-22T02:51:46.995045Z"
    },
    "id": "T5cfV2kMvBQf"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        text = text[: int(len(text) / 20)]\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        chunk = self.encoded[idx: idx + self.chunk_size]\n",
    "        label = self.encoded[idx + 1: idx + self.chunk_size + 1]\n",
    "\n",
    "        return chunk, label\n",
    "\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)]\n",
    "    )\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mg9d9W_UvBQf"
   },
   "source": [
    "注意力的计算公式为：\n",
    "\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "\n",
    "这里实现的一些数学技巧可以参见 attention.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.098514Z",
     "start_time": "2024-06-22T02:51:47.087226Z"
    },
    "id": "iHI-eCKFvBQf"
   },
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len: int, embed_size: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.to_q.weight)\n",
    "        nn.init.xavier_uniform_(self.to_k.weight)\n",
    "        nn.init.xavier_uniform_(self.to_v.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        inputs.to(device)\n",
    "        Q = self.to_q(inputs)\n",
    "        K = self.to_k(inputs)\n",
    "        V = self.to_v(inputs)\n",
    "        Attention = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
    "        Attention = Attention.masked_fill(self.tril == 0, float(\"-inf\"))\n",
    "        Attention = F.softmax(Attention, dim=-1)\n",
    "        Attention = torch.matmul(Attention, V).to(inputs.device)\n",
    "        return Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsVWVtiUvBQg"
   },
   "source": [
    "Transformer 中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "\n",
    "在搭建网络的过程中，同学们可能会用到 nn.ModuleList 这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.127277Z",
     "start_time": "2024-06-22T02:51:47.110288Z"
    },
    "id": "WS5NY6TbvBQg"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, head_size: int, seq_len: int, embed_size: int):\n",
    "        super().__init__()\n",
    "        head_size = embed_size // n_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [HeadAttention(seq_len, embed_size, head_size) for _ in range(n_heads)]\n",
    "        )\n",
    "        self.projection = nn.Linear(embed_size, embed_size)\n",
    "        nn.init.xavier_uniform_(self.projection.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs.to(device)\n",
    "        head_outputs = [head(inputs) for head in self.heads]\n",
    "        MHAttention = torch.cat(head_outputs, dim=-1)\n",
    "        MHAttention = self.projection(MHAttention).to(inputs.device)\n",
    "        return MHAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQhoHEJ_vBQg"
   },
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert 即为标准 Transformer 中的 FeedForward 模块。\n",
    "\n",
    "在经过 MultiHeadAttention 模块后，seq_len 中的每一个 Embedding 都对应了前文信息的加权求和。在经过 FeedForward 模块时，模型对每一个位置的 Embedding 进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于 MultiHeadAttention，我们在每一层训练多个 FeedForward 模块，对于不同位置的 Embedding 使用不同的 FeedForward 模块处理对应的信息。就好像每层有多个 Expert,每个 Expert 都负责处理一类数据的深加工，因此我们称 FeedForward 为 Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward 层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量 x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将 x 最后一维扩大至原先 4 倍，然后继续用一个线性层还原回原先的维度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.145530Z",
     "start_time": "2024-06-22T02:51:47.133369Z"
    },
    "id": "Q0U_WYPCvBQg"
   },
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, embed_size * 4)\n",
    "        self.fc2 = nn.Linear(embed_size * 4, embed_size)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs.to(device)\n",
    "        mid = F.relu(self.fc1(inputs))\n",
    "        outputs = self.fc2(mid).to(inputs.device)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHcYkH9ivBQg"
   },
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个 Expert 后，我们要设计一个选通网络决策每个 Embedding 要使用那个 Expert 计算\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16]\n",
    "\n",
    "即输入有 batch_size=1 个数据点，该数据有 seq_len 长度的 context，即包含 seq_len=8 个 Embedding，每个 Embedding 长度为 embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的 Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个 Embedding 仅有 active_expert 个 Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "\n",
    "对于有 seq_len=8 的数据，如果每个 Expert 都参与计算每一个 Embedding，那么一共需要计算 seq_len\\*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的 active_experts 个 Expert，这要求我们对每一个 Embedding 计算最合适的 active_experts 个 Expert。\n",
    "\n",
    "对于单个 Expert 的原版 Transformer 来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个 Expert 的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通 0,2 号 Expert 的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个 Embedding ，我们使用神经网络对每个 Expert 打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前 k 大的分数的下标（即 argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.299418Z",
     "start_time": "2024-06-22T02:51:47.276316Z"
    },
    "id": "jCxoVAx2vBQg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_experts),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs.to(device)\n",
    "        score = self.mlp(inputs)\n",
    "        indices = torch.topk(score, self.active_experts, dim=-1)[1]\n",
    "        mask = torch.zeros_like(score, dtype=torch.bool)\n",
    "        mask.scatter_(dim=-1, index=indices, value=True)\n",
    "\n",
    "        score = score.masked_fill(~mask, float(\"-inf\"))\n",
    "        router_output = F.softmax(score, dim=-1)\n",
    "        router_output = router_output.to(inputs.device)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OejgYVCvBQg",
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完 Expert 和 TopkRouter 后，我们可以定义 SparseMoE 模块。\n",
    "\n",
    "在前向过程中，对于 inputs.shape = [Batch_size,seq_len,embed_size]第二维度 seq_len 个 Embedding,我们先利用 TopkRouter 计算出选通专家序号 indices 以及专家权重 router_output。\n",
    "\n",
    "我们将 Embedding 通过选通的 Expert 得出 active_expert 个新的 Embedding，然后使用 router_output 的作为权重对新的 Embedding 加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.360869Z",
     "start_time": "2024-06-22T02:51:47.354007Z"
    },
    "id": "22w81qg3vBQg"
   },
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_experts: int, active_experts: int):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        router_output, indices = self.router(inputs)\n",
    "\n",
    "        batch_size, seq_len, _ = inputs.size()\n",
    "        final_output = torch.zeros_like(inputs).to(inputs.device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                token_inputs = inputs[b, s]\n",
    "\n",
    "                expert_outputs = []\n",
    "                for idx in indices[b, s]:\n",
    "                    output = self.experts[idx](token_inputs.to(inputs.device))\n",
    "                    expert_outputs.append(output)\n",
    "\n",
    "                expert_outputs = torch.stack(expert_outputs).to(inputs.device)\n",
    "                token_router_output = router_output[b, s][indices[b, s]].to(\n",
    "                    inputs.device\n",
    "                )\n",
    "                weighted_sum = torch.sum(\n",
    "                    token_router_output.unsqueeze(-1) * expert_outputs,\n",
    "                    dim=0,\n",
    "                )\n",
    "                final_output[b, s] = weighted_sum\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZrcAdgqvBQg"
   },
   "source": [
    "Transformer 由一层层的 block 堆叠而成，其中每个 block 的结构从模型的结构图展开中可以看到，由 LayerNorm，Masked multi head attention，(SparseMoE)FeedForward 组成。\n",
    "\n",
    "对于一个表示句子的输入向量 x，其首先会经过 Layer Normalization 层.\n",
    "Layer Normalization 层对于一个 句子个数 x 句子长度 x 单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中 mean 和 var 都是在最后两个维度上进行的，layernorm 的实现同学们可以直接调用 nn.LayerNorm\n",
    "经过 layernorm 层后，再经过 Mask multi head attention 层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层 layernorm 和 feedforwad 之后，就可以得到 block 块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.398133Z",
     "start_time": "2024-06-22T02:51:47.392167Z"
    },
    "id": "vl654g47vBQh"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size: int,\n",
    "        n_heads: int,\n",
    "        seq_len: int,\n",
    "        num_experts: int,\n",
    "        active_experts: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_size)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_size)\n",
    "        self.attention = MultiHeadAttention(\n",
    "            n_heads, embed_size // n_heads, seq_len, embed_size\n",
    "        )\n",
    "        self.moe = SparseMoE(embed_size, num_experts, active_experts)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layernorm1(inputs)\n",
    "        x = x + self.attention(x)\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.moe(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.439350Z",
     "start_time": "2024-06-22T02:51:47.422351Z"
    },
    "id": "lzKme81ZvBQh"
   },
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        embed_size: int,\n",
    "        n_layers: int,\n",
    "        n_heads: int,\n",
    "        num_experts: int,\n",
    "        active_experts: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(embed_size, n_heads, seq_len, num_experts, active_experts)\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm = nn.LayerNorm(embed_size)\n",
    "        self.output_linear = nn.Linear(embed_size, vocab_size)\n",
    "        nn.init.xavier_uniform_(self.output_linear.weight)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        (batch_size, seq_len,) = inputs.shape\n",
    "\n",
    "        embedding = self.token_embedding(inputs) + self.position_embedding(\n",
    "            torch.arange(seq_len, device=inputs.device)\n",
    "        )\n",
    "\n",
    "        for block in self.blocks:\n",
    "            embedding = block(embedding)\n",
    "\n",
    "        logits = self.output_linear(self.layernorm(embedding))\n",
    "\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, seq_len, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * seq_len, vocab_size)\n",
    "            labels = labels.view(batch_size * seq_len)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        inputs = tokenizer.encode(inputs).clone().detach().unsqueeze(0)\n",
    "        device = next(self.parameters()).device\n",
    "        inputs = inputs.to(device)\n",
    "        if inputs.size(1) > self.seq_len:\n",
    "            inputs = inputs[:, : self.seq_len]\n",
    "        generated = inputs\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)\n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)\n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)\n",
    "            tmp = generated.clone().detach()\n",
    "        return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWDSdJiVvBQh"
   },
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aARWjavNvBQh"
   },
   "source": [
    "#### Loss\n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "-   交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "-   均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "-   绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mv6xZ-ivBQh"
   },
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "-   从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "-   将数据送入模型，进行前向传播\n",
    "-   拿到模型输出的 logits\n",
    "-   将 logits 和 标签进行 loss 计算\n",
    "-   用 Optimizer\n",
    "    -   清空梯度\n",
    "    -   反向传播\n",
    "    -   更新参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T02:51:47.448623Z",
     "start_time": "2024-06-22T02:51:47.442341Z"
    },
    "id": "TMBFTQjavBQh"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.clone().detach()\n",
    "        targets = targets.clone().detach()\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss / len(dataloader)}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        logits, loss = model(inputs, targets)\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} Loss: {total_loss / len(dataloader)}\")\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T03:25:46.518314Z",
     "start_time": "2024-06-22T02:51:47.451533Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_dataloader, val_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=50, batch_size=512)\n",
    "model = SparseMoETransformer(vocab_size=len(tokenizer.char2index), seq_len=50, embed_size=64, n_layers=1, n_heads=8, num_experts=8, active_experts=2).to(device)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "train_dataloader, val_dataloader = create_dataloader(\n",
    "    \"input.txt\",\n",
    "    tokenizer,\n",
    "    chunk_size=20,\n",
    "    batch_size=10,\n",
    ")\n",
    "model = SparseMoETransformer(\n",
    "    vocab_size=len(tokenizer.char2index),\n",
    "    seq_len=20,\n",
    "    embed_size=4,\n",
    "    n_layers=1,\n",
    "    n_heads=1,\n",
    "    num_experts=2,\n",
    "    active_experts=1,\n",
    ").to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=0):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        print(f\"Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}\")\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "    return train_losses, valid_losses\n",
    "\n",
    "\n",
    "def plot_loss(train_loss, valid_loss):\n",
    "    plt.plot(train_loss, label=\"train loss\")\n",
    "    plt.plot(valid_loss, label=\"valid loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_losses, valid_losses = run(\n",
    "    model, train_dataloader, val_dataloader, device, epochs=15\n",
    ")\n",
    "plot_loss(train_losses, valid_losses)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(\"I could pick my lance\", max_new_tokens=100)[0].tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
